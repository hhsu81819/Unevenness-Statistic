{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b13eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import xarray as xr\n",
    "import sys\n",
    "import scipy.io\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "from os import path\n",
    "import netCDF4\n",
    "from statsmodels.tsa.stattools import acf\n",
    "import netCDF4 as nc\n",
    "import warnings\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.datasets import make_blobs \n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.integrate import trapz\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cmap = plt.cm.RdBu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3463398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def morani(p):\n",
    "    # Replace this with your actual 2D data of shape (192, 288)\n",
    "\n",
    "    # Remove NaN values\n",
    "    p_clean = p[~np.isnan(p).any(axis=1)]\n",
    "\n",
    "    # Calculate mean of the cleaned data\n",
    "    mean_p = np.mean(p_clean)\n",
    "\n",
    "    # Number of data points\n",
    "    n = p_clean.shape[0]\n",
    "\n",
    "    # Calculate pairwise distances between data points\n",
    "    distances = squareform(pdist(p_clean))\n",
    "\n",
    "    # Define a threshold distance for spatial weights\n",
    "    threshold = 1.0  # Adjust this based on your data characteristics\n",
    "\n",
    "    # Create binary spatial weights matrix based on distance threshold\n",
    "    w = np.where(distances <= threshold, 1, 0)\n",
    "\n",
    "    # Calculate sums for Moran's I formula\n",
    "    s1 = np.sum(w)\n",
    "    s2 = np.dot(w, p_clean)\n",
    "    s3 = p_clean - mean_p\n",
    "\n",
    "    # Calculate Moran's I\n",
    "    moran_i = (n / s1) * (np.dot(s2.T, s3) / np.dot(s3.T, s3))\n",
    "\n",
    "    return moran_i\n",
    "\n",
    "\n",
    "def gini(x):\n",
    "    total = 0\n",
    "    for i, xi in enumerate(x[:-1], 1):\n",
    "        total += np.sum(np.abs(xi - x[i:]))\n",
    "    return total / (len(x)**2 * np.mean(x))\n",
    "\n",
    "def weighted_percentile(cli_pr, weights, percentile):\n",
    "    # Flatten the 2D arrays and remove NaN values\n",
    "    flat_array = cli_pr.flatten()\n",
    "    valid_indices = np.logical_not(np.isnan(flat_array))\n",
    "    sorted_indices = np.argsort(flat_array[valid_indices])\n",
    "    sorted_array = flat_array[valid_indices][sorted_indices]\n",
    "    sorted_weights = weights.flatten()[valid_indices][sorted_indices]\n",
    "    \n",
    "    # Calculate the cumulative sum of the sorted weights\n",
    "    cumulative_weights = np.cumsum(sorted_weights)\n",
    "    total_weight = cumulative_weights[-1]\n",
    "    \n",
    "    # Find the index corresponding to the desired percentile\n",
    "    target_index = np.searchsorted(cumulative_weights, total_weight * (percentile / 100.0))\n",
    "    \n",
    "    # Get the value at the target index in the sorted array\n",
    "    percentile_value = sorted_array[target_index]\n",
    "    \n",
    "    return percentile_value\n",
    "\n",
    "\n",
    "def giniw(x, w=None):\n",
    "# from https://stackoverflow.com/questions/48999542/more-efficient-weighted-gini-coefficient-in-python\n",
    "    # The rest of the code requires numpy arrays.\n",
    "#     x = np.asarray(x)\n",
    "    x=x.flatten()\n",
    "    w=w.flatten()\n",
    "    if w is not None:\n",
    "#         w = np.asarray(w)\n",
    "        sorted_indices = np.argsort(x)\n",
    "        sorted_x = x[sorted_indices]\n",
    "        sorted_w = w[sorted_indices]\n",
    "        # Force float dtype to avoid overflows\n",
    "        cumw = np.cumsum(sorted_w)\n",
    "        cumxw = np.cumsum(sorted_x * sorted_w)\n",
    "        return (np.sum(cumxw[1:] * cumw[:-1] - cumxw[:-1] * cumw[1:]) /\n",
    "                (cumxw[-1] * cumw[-1]))\n",
    "    else:\n",
    "#         x = np.asarray(x)\n",
    "        sorted_x = np.sort(x)\n",
    "        n = x.size\n",
    "        cumx = np.cumsum(sorted_x)\n",
    "        # The above formula, with all weights equal to 1 simplifies to:\n",
    "        return (n + 1 - 2 * np.sum(cumx) / cumx[-1]) / n\n",
    "    \n",
    "def find_weighted_percentile_rank(cli_pr, weights, value):\n",
    "    flat_array = cli_pr.flatten()\n",
    "    flat_weights = weights.flatten()\n",
    "    \n",
    "    # Sort the values and corresponding weights\n",
    "    sorted_indices = np.argsort(flat_array)\n",
    "    sorted_array = flat_array[sorted_indices]\n",
    "    sorted_weights = flat_weights[sorted_indices]\n",
    "    \n",
    "    # Calculate the cumulative sum of the sorted weights\n",
    "    cumulative_weights = np.cumsum(sorted_weights)\n",
    "    total_weight = cumulative_weights[-1]\n",
    "    \n",
    "    # Find the index of the value in the sorted array\n",
    "    index = np.searchsorted(sorted_array, value)\n",
    "    \n",
    "    # Calculate the percentile rank considering weight\n",
    "    percentile_rank = (cumulative_weights[index] / total_weight) * 100\n",
    "    \n",
    "    return percentile_rank\n",
    "\n",
    "def calculate_mann_kendall(x):\n",
    "    n = len(x)\n",
    "    s = 0\n",
    "    for k in range(n - 1):\n",
    "        for j in range(k + 1, n):\n",
    "            s += np.sign(x[j] - x[k])\n",
    "    trend = s / (n * (n - 1) / 2)\n",
    "    var_s = (n * (n - 1) * (2 * n + 5)) / 18\n",
    "    if s > 0:\n",
    "        z = (s - 1) / np.sqrt(var_s)\n",
    "    elif s < 0:\n",
    "        z = (s + 1) / np.sqrt(var_s)\n",
    "    else:\n",
    "        z = 0\n",
    "    p = 2 * (1 - norm.cdf(abs(z)))\n",
    "    return trend, p\n",
    "\n",
    "def spa_lag(X, k):\n",
    "\n",
    "    tree = cKDTree(X)\n",
    "    spatial_lag = []\n",
    "\n",
    "    for point in X:\n",
    "        # Query the k nearest neighbors' indices\n",
    "        _, indices = tree.query(point, k=k+1)  # +1 to exclude the point itself\n",
    "\n",
    "        # Calculate the spatial lag by averaging the values of the k nearest neighbors\n",
    "        spatial_lag_value = np.mean(X[indices[1:], :], axis=0)  # Exclude the first index which is the point itself\n",
    "        spatial_lag.append(spatial_lag_value)\n",
    "\n",
    "    spatial_lag = np.array(spatial_lag)\n",
    "    return spatial_lag\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def spa_lag_with_weights(X, W, k):\n",
    "    tree = cKDTree(X)\n",
    "    spatial_lag = []\n",
    "\n",
    "    for i, point in enumerate(X):\n",
    "        # Query the k nearest neighbors' indices\n",
    "        _, indices = tree.query(point, k=k+1)  # +1 to exclude the point itself\n",
    "\n",
    "        # Get the weights corresponding to the k nearest neighbors\n",
    "        neighbor_weights = W[indices[1:]]\n",
    "\n",
    "        # Normalize the weights\n",
    "        normalized_weights = neighbor_weights / np.sum(neighbor_weights)\n",
    "\n",
    "        # Calculate the spatial lag using weighted average without np.dot\n",
    "        spatial_lag_value = np.sum(normalized_weights * X[indices[1:], :], axis=0)\n",
    "        spatial_lag.append(spatial_lag_value)\n",
    "\n",
    "    spatial_lag = np.array(spatial_lag)\n",
    "    return spatial_lag\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# X is your data matrix\n",
    "# W is your spatial weight matrix\n",
    "# k is the number of nearest neighbors to consider\n",
    "# result = spa_lag_with_weights(X, W, k)\n",
    "\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "def calculate_local_moran(p, w):\n",
    "    p = p.flatten()\n",
    "    w = w.flatten()\n",
    "    n = p.shape[0]\n",
    "    mean_p = np.mean(p)\n",
    "    variance_p = np.var(p)\n",
    "    \n",
    "    local_morans_i_values = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        num = n * (p[i] - mean_p) * np.sum(w[i] * (p - mean_p))\n",
    "        den = variance_p\n",
    "        local_morans_i_values[i] = num / den\n",
    "        \n",
    "    return local_morans_i_values\n",
    "\n",
    "# Example data and weights (replace with your actual data and weights)\n",
    "\n",
    "\n",
    "\n",
    "def sgini(pts, w):\n",
    "    w=w/np.nansum(w)\n",
    "    sorted_indices = np.argsort(pts.flatten())\n",
    "    sorted_x = pts.flatten()[sorted_indices]\n",
    "    sorted_w = w.flatten()[sorted_indices]\n",
    "    # Force float dtype to avoid overflows\n",
    "    cumw = np.cumsum(sorted_w)-sorted_w/2\n",
    "    cumx = np.cumsum(sorted_x)\n",
    "    cumx=cumx/cumx[-1]\n",
    "    x_clipped = np.clip(cumw/cumw[-1], 0, 1)\n",
    "    area = trapz(cumx[x_clipped <= 1], x_clipped[x_clipped <= 1])   \n",
    "    GS=(0.5-area)*2    \n",
    "    return GS\n",
    "\n",
    "def sgini_la(pts, la, w):\n",
    "    w=w/np.nansum(w)\n",
    "\n",
    "    sorted_indicesr = np.argsort(la.flatten())\n",
    "\n",
    "    sorted_indices = np.argsort(pts.flatten())\n",
    "    sorted_x = pts.flatten()[sorted_indicesr]\n",
    "    sorted_w = w.flatten()[sorted_indices]\n",
    "    # Force float dtype to avoid overflows\n",
    "    cumw = np.cumsum(sorted_w)-sorted_w/2\n",
    "    cumx = np.cumsum(sorted_x)\n",
    "    cumx=cumx/cumx[-1]\n",
    "    x_clipped = np.clip(cumw/cumw[-1], 0, 1)\n",
    "    area = trapz(cumx[x_clipped <= 1], x_clipped[x_clipped <= 1]) \n",
    "    GS=(0.5-area)*2\n",
    "    return GS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def random_rearrange_2d_array(p):\n",
    "    # Flatten the 2D array into a 1D array\n",
    "    flattened_p = p.flatten()\n",
    "    \n",
    "    # Shuffle the elements of the 1D array randomly\n",
    "    np.random.shuffle(flattened_p)\n",
    "    \n",
    "    # Reshape the shuffled 1D array back into the original 2D array shape\n",
    "    reshaped_p = flattened_p.reshape(p.shape)\n",
    "    \n",
    "    return reshaped_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4879efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob\n",
    "MODELINFO=[[\"ACCESS-ESM1-5\",101,249],\\\n",
    "           [\"CanESM5\",1850,2000],\\\n",
    "           [\"CESM2\",1,150],\\\n",
    "           [\"EC-Earth3-CC\",1850,2000],\\\n",
    "           [\"FGOALS-g3\",370,525],\\\n",
    "           [\"MIROC6\",3200,3349],\\\n",
    "           [\"MRI-ESM2-0\",1850,2000],\\\n",
    "           [\"NorESM2-LM\",1,150],\\\n",
    "           [\"NorESM2-MM\",1,150],\\\n",
    "           [\"GFDL-ESM4\",1,150],\\\n",
    "           [\"GFDL-CM4\",1,150]]\n",
    "\n",
    "GRIDINFO=[[\"/pr_day_ACCESS-ESM1-5_1pctCO2_r1i1p1f1_gn_01510101-02001231.nc\"],\\\n",
    "          [\"/pr_day_CanESM5_1pctCO2_r1i1p1f1_gn_18500101-20001231.nc\"],\\\n",
    "          [\"/pr_day_CESM2_1pctCO2_r1i1p1f1_gn_00710101-00801231.nc\"],\\\n",
    "          [\"/pr_day_EC-Earth3-CC_1pctCO2_r1i1p1f1_gr_19080101-19081231.nc\"],\\\n",
    "          [\"/pr_day_FGOALS-g3_1pctCO2_r1i1p1f1_gn_05160101-05161231.nc\"],\\\n",
    "          [\"/pr_day_MIROC6_1pctCO2_r1i1p1f1_gn_32800101-32891231.nc\"],\\\n",
    "          [\"/pr_day_MRI-ESM2-0_1pctCO2_r1i1p1f1_gn_18500101-18991231.nc\"],\\\n",
    "          [\"/pr_day_NorESM2-LM_1pctCO2_r1i1p1f1_gn_01210101-01301231.nc\"],\\\n",
    "          [\"/pr_day_NorESM2-MM_1pctCO2_r1i1p1f1_gn_01210101-01301231.nc\"],\\\n",
    "          [\"/pr_day_GFDL-ESM4_1pctCO2_r1i1p1f1_gr1_00610101-00801231.nc\"],\\\n",
    "          [\"/pr_day_GFDL-CM4_1pctCO2_r1i1p1f1_gr1_00210101-00401231.nc\"]]\n",
    "\n",
    "MASKINFO=[[\"/mrsos_day_ACCESS-ESM1-5_1pctCO2_r1i1p1f1_gn_01010101-01501231.nc\"],\\\n",
    "          [\"/mrsos_Lmon_CanESM5_1pctCO2_r1i1p1f1_gn_185001-200012.nc\"],\\\n",
    "          [\"/mrsos_Lmon_CESM2_1pctCO2_r1i1p1f1_gn_000101-005012.nc\"],\\\n",
    "          [\"/mrsos_Lmon_EC-Earth3-CC_1pctCO2_r1i1p1f1_gr_199601-199612.nc\"],\\\n",
    "          [\"/mrsos_Lmon_FGOALS-g3_1pctCO2_r1i1p1f1_gn_037301-052812.nc\"],\\\n",
    "          [\"/mrsos_Lmon_MIROC6_1pctCO2_r1i1p1f1_gn_320001-329912.nc\"],\\\n",
    "          [\"/mrsos_Lmon_MRI-ESM2-0_1pctCO2_r1i1p1f1_gn_185001-200012.nc\"],\\\n",
    "          [\"/mrsos_Lmon_NorESM2-LM_1pctCO2_r1i1p1f1_gn_007101-008012.nc\"],\\\n",
    "          [\"/mrsos_Lmon_NorESM2-MM_1pctCO2_r1i1p1f1_gn_007101-008012.nc\"],\\\n",
    "          [\"/mrsos_Lmon_GFDL-ESM4_1pctCO2_r1i1p1f1_gr1_010101-015012.nc\"],\\\n",
    "          [\"/mrsos_Lmon_GFDL-CM4_1pctCO2_r1i1p1f1_gr1_000101-010012.nc\"]]\n",
    "\n",
    "model_names = [model[0] for model in MODELINFO]\n",
    "model_start = [model[1] for model in MODELINFO]\n",
    "model_end = [model[2] for model in MODELINFO]\n",
    "GRID = [model[0] for model in GRIDINFO]\n",
    "MASK = [model[0] for model in MASKINFO]\n",
    "lenyear=100\n",
    "miss_val = -9.99e08\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb8c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latuse = 30\n",
    "\n",
    "for model in range(0,10):\n",
    "    GINIland=np.array([np.full((lenyear),miss_val)])\n",
    "    GINIsland=np.array([np.full((lenyear),miss_val)])\n",
    "\n",
    "    GINIocean=np.array([np.full((lenyear),miss_val)])\n",
    "    GINIsocean=np.array([np.full((lenyear),miss_val)])\n",
    "\n",
    "    GINIglob=np.array([np.full((lenyear),miss_val)])\n",
    "    GINIsglob=np.array([np.full((lenyear),miss_val)])\n",
    "  \n",
    "    MoranI=np.array([np.full((lenyear),miss_val)])\n",
    "    \n",
    "    mat1 = nc.Dataset('/scratch/cimes/hh9736/CMIP6_pr/' + model_names[model] + GRID[model])\n",
    "    data1 = nc.Dataset('/scratch/cimes/hh9736/CMIP6_pr/' + model_names[model] + MASK[model])\n",
    "    data2 = xr.open_dataset('/scratch/cimes/hh9736/CMIP6_pr/' + model_names[model] + MASK[model])\n",
    "    lats = data2['lat'].values\n",
    "    lats = lats.astype(int)\n",
    "    expolar = np.logical_or(lats > latuse, lats < -latuse)\n",
    "    \n",
    "    kn=round(lats.shape[0]/18)\n",
    "    \n",
    "    latmax=np.size(mat1['pr'][1,:,1])+1\n",
    "    lonmax=np.size(mat1['pr'][1,1,:])+1\n",
    "    lenlat=latmax-1\n",
    "    lenlon=lonmax-1\n",
    "    lenyear=2100-1850+1\n",
    "    miss_val = -9.99e08\n",
    "\n",
    "    lats = data2['lat'].values\n",
    "    lons = data2['lon'].values\n",
    "    weights = np.cos(np.deg2rad(lats))\n",
    "    weights /= np.sum(weights)\n",
    "    weights_reshaped = np.reshape(weights, (1, len(lats), 1))\n",
    "    weights_reshaped = np.repeat(weights_reshaped, len(lons), axis=2)\n",
    "\n",
    "\n",
    "# Specify the path to your NetCDF files\n",
    "    file_pattern = \"/scratch/cimes/hh9736/CMIP6_pr/\"+ model_names[model] +\"/pr*_r1i1p1f1_*.nc\"\n",
    "\n",
    "    # Get a list of file paths matching the pattern\n",
    "    file_paths = glob.glob(file_pattern)\n",
    "\n",
    "    # Open and concatenate the files one by one\n",
    "    datasets = []\n",
    "    for file_path in file_paths:\n",
    "        dataset = xr.open_dataset(file_path)\n",
    "        datasets.append(dataset)\n",
    "\n",
    "    # Concatenate the datasets along the time dimension\n",
    "    data = xr.concat(datasets, dim=\"time\")\n",
    "    data = data.sortby('time')\n",
    "    annual_mean = data.groupby('time.year').mean(dim='time')\n",
    "\n",
    "    # Select the first 100 years of data\n",
    "    start_year = annual_mean.year[0]\n",
    "    end_year = start_year + 99\n",
    "    YEARP = annual_mean.sel(year=slice(start_year, end_year))\n",
    "    mask=np.array(data1['mrsos'])\n",
    "    \n",
    "    mask=mask[0,:,:]\n",
    "    mask[mask>10000]=1.e+20\n",
    "    mask[mask<0.000000000000000000000001]=1.e+20\n",
    "   \n",
    "    weights_reshaped=weights_reshaped[0,:,:]\n",
    "    \n",
    "    mat1=YEARP['pr']\n",
    "    for yy in range(0,100):\n",
    "        pts=np.array(mat1[yy,:,:])\n",
    "  \n",
    "        la=spa_lag(pts, kn)\n",
    "        pts[expolar, :] = np.nan\n",
    "        \n",
    "        pts_land=copy(pts)\n",
    "        pts_land[mask>10000]=np.nan\n",
    "        GINIland[0,yy]=sgini(pts_land[~np.isnan(pts_land)], weights_reshaped[~np.isnan(pts_land)])\n",
    "        GINIsland[0,yy]=sgini_la(pts_land[~np.isnan(pts_land)],la[~np.isnan(pts_land)], weights_reshaped[~np.isnan(pts_land)])\n",
    "        \n",
    "        pts_ocean=copy(pts)\n",
    "        pts_ocean[mask<10000]=np.nan\n",
    "        GINIocean[0,yy]=sgini(pts_ocean[~np.isnan(pts_ocean)], weights_reshaped[~np.isnan(pts_ocean)])\n",
    "        GINIsocean[0,yy]=sgini_la(pts_ocean[~np.isnan(pts_ocean)],la[~np.isnan(pts_ocean)], weights_reshaped[~np.isnan(pts_ocean)])\n",
    "        \n",
    "        pts_glob=copy(pts)\n",
    "        GINIglob[0,yy]=sgini(pts_glob[~np.isnan(pts_glob)], weights_reshaped[~np.isnan(pts_glob)])\n",
    "        GINIsglob[0,yy]=sgini_la(pts_glob[~np.isnan(pts_glob)],la[~np.isnan(pts_glob)], weights_reshaped[~np.isnan(pts_glob)])\n",
    "        \n",
    "        \n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINI_spa_'+ str(latuse) +'landP_' + model_names[model] + '.npy', np.array([GINIland]))\n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINIs_spa_'+ str(latuse) +'landP_' + model_names[model] + '.npy', np.array([GINIsland]))\n",
    "    \n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINI_spa_'+ str(latuse) +'oceanP_' + model_names[model] + '.npy', np.array([GINIocean]))\n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINIs_spa_'+ str(latuse) +'oceanP_' + model_names[model] + '.npy', np.array([GINIsocean]))\n",
    "    \n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINI_spa_'+ str(latuse) +'globP_' + model_names[model] + '.npy', np.array([GINIglob]))\n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINIs_spa_'+ str(latuse) +'globP_' + model_names[model] + '.npy', np.array([GINIsglob]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8bd658",
   "metadata": {},
   "outputs": [],
   "source": [
    "latuse = 90\n",
    "latnouse = 30\n",
    "\n",
    "for model in range(0,11):\n",
    "    GINIland=np.array([np.full((lenyear),miss_val)])\n",
    "    GINIsland=np.array([np.full((lenyear),miss_val)])\n",
    "\n",
    "    GINIocean=np.array([np.full((lenyear),miss_val)])\n",
    "    GINIsocean=np.array([np.full((lenyear),miss_val)])\n",
    "\n",
    "    GINIglob=np.array([np.full((lenyear),miss_val)])\n",
    "    GINIsglob=np.array([np.full((lenyear),miss_val)])\n",
    "  \n",
    "    MoranI=np.array([np.full((lenyear),miss_val)])\n",
    "    \n",
    "    mat1 = nc.Dataset('/scratch/cimes/hh9736/CMIP6_pr/' + model_names[model] + GRID[model])\n",
    "    data1 = nc.Dataset('/scratch/cimes/hh9736/CMIP6_pr/' + model_names[model] + MASK[model])\n",
    "    data2 = xr.open_dataset('/scratch/cimes/hh9736/CMIP6_pr/' + model_names[model] + MASK[model])\n",
    "    lats = data2['lat'].values\n",
    "    lats = lats.astype(int)\n",
    "    expolar = np.logical_and(lats > -latnouse, lats < latnouse)\n",
    "    \n",
    "    kn=round(lats.shape[0]/18)\n",
    "    \n",
    "    latmax=np.size(mat1['pr'][1,:,1])+1\n",
    "    lonmax=np.size(mat1['pr'][1,1,:])+1\n",
    "    lenlat=latmax-1\n",
    "    lenlon=lonmax-1\n",
    "    lenyear=2100-1850+1\n",
    "    miss_val = -9.99e08\n",
    "\n",
    "    lats = data2['lat'].values\n",
    "    lons = data2['lon'].values\n",
    "    weights = np.cos(np.deg2rad(lats))\n",
    "    weights /= np.sum(weights)\n",
    "    weights_reshaped = np.reshape(weights, (1, len(lats), 1))\n",
    "    weights_reshaped = np.repeat(weights_reshaped, len(lons), axis=2)\n",
    "\n",
    "\n",
    "# Specify the path to your NetCDF files\n",
    "    file_pattern = \"/scratch/cimes/hh9736/CMIP6_pr/\"+ model_names[model] +\"/pr*_r1i1p1f1_*.nc\"\n",
    "\n",
    "    # Get a list of file paths matching the pattern\n",
    "    file_paths = glob.glob(file_pattern)\n",
    "\n",
    "    # Open and concatenate the files one by one\n",
    "    datasets = []\n",
    "    for file_path in file_paths:\n",
    "        dataset = xr.open_dataset(file_path)\n",
    "        datasets.append(dataset)\n",
    "\n",
    "    # Concatenate the datasets along the time dimension\n",
    "    data = xr.concat(datasets, dim=\"time\")\n",
    "    data = data.sortby('time')\n",
    "    annual_mean = data.groupby('time.year').mean(dim='time')\n",
    "\n",
    "\n",
    "    # Select the first 100 years of data\n",
    "    start_year = annual_mean.year[0]\n",
    "    end_year = start_year + 99\n",
    "    YEARP = annual_mean.sel(year=slice(start_year, end_year))\n",
    "    mask=np.array(data1['mrsos'])\n",
    "    \n",
    "    mask=mask[0,:,:]\n",
    "    mask[mask>10000]=1.e+20\n",
    "    mask[mask<0.000000000000000000000001]=1.e+20\n",
    "   \n",
    "    weights_reshaped=weights_reshaped[0,:,:]\n",
    "    \n",
    "    mat1=YEARP['pr']\n",
    "    for yy in range(0,100):\n",
    "        pts=np.array(mat1[yy,:,:])\n",
    "\n",
    "        la=spa_lag(pts, kn)\n",
    "        pts[expolar, :] = np.nan\n",
    "\n",
    "        pts_land=copy(pts)\n",
    "        pts_land[mask>10000]=np.nan\n",
    "        GINIland[0,yy]=sgini(pts_land[~np.isnan(pts_land)], weights_reshaped[~np.isnan(pts_land)])\n",
    "        GINIsland[0,yy]=sgini_la(pts_land[~np.isnan(pts_land)],la[~np.isnan(pts_land)], weights_reshaped[~np.isnan(pts_land)])\n",
    "        \n",
    "        pts_ocean=copy(pts)\n",
    "        pts_ocean[mask<10000]=np.nan\n",
    "        GINIocean[0,yy]=sgini(pts_ocean[~np.isnan(pts_ocean)], weights_reshaped[~np.isnan(pts_ocean)])\n",
    "        GINIsocean[0,yy]=sgini_la(pts_ocean[~np.isnan(pts_ocean)],la[~np.isnan(pts_ocean)], weights_reshaped[~np.isnan(pts_ocean)])\n",
    "        \n",
    "        pts_glob=copy(pts)\n",
    "        GINIglob[0,yy]=sgini(pts_glob[~np.isnan(pts_glob)], weights_reshaped[~np.isnan(pts_glob)])\n",
    "        GINIsglob[0,yy]=sgini_la(pts_glob[~np.isnan(pts_glob)],la[~np.isnan(pts_glob)], weights_reshaped[~np.isnan(pts_glob)])\n",
    "        \n",
    "        \n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINI_spa_'+ str(latnouse) +''+ str(latuse) +'landP_' + model_names[model] + '.npy', np.array([GINIland]))\n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINIs_spa_'+ str(latnouse) +''+ str(latuse) +'landP_' + model_names[model] + '.npy', np.array([GINIsland]))\n",
    "    \n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINI_spa_'+ str(latnouse) +''+ str(latuse) +'oceanP_' + model_names[model] + '.npy', np.array([GINIocean]))\n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINIs_spa_'+ str(latnouse) +''+ str(latuse) +'oceanP_' + model_names[model] + '.npy', np.array([GINIsocean]))\n",
    "    \n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINI_spa_'+ str(latnouse) +''+ str(latuse) +'globP_' + model_names[model] + '.npy', np.array([GINIglob]))\n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINIs_spa_'+ str(latnouse) +''+ str(latuse) +'globP_' + model_names[model] + '.npy', np.array([GINIsglob]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d474237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latuse = 90\n",
    "\n",
    "for model in range(0,10):\n",
    "    GINIland=np.array([np.full((lenyear),miss_val)])\n",
    "    GINIsland=np.array([np.full((lenyear),miss_val)])\n",
    "\n",
    "    GINIocean=np.array([np.full((lenyear),miss_val)])\n",
    "    GINIsocean=np.array([np.full((lenyear),miss_val)])\n",
    "\n",
    "    GINIglob=np.array([np.full((lenyear),miss_val)])\n",
    "    GINIsglob=np.array([np.full((lenyear),miss_val)])\n",
    "  \n",
    "    MoranI=np.array([np.full((lenyear),miss_val)])\n",
    "    \n",
    "    mat1 = nc.Dataset('/scratch/cimes/hh9736/CMIP6_pr/' + model_names[model] + GRID[model])\n",
    "    data1 = nc.Dataset('/scratch/cimes/hh9736/CMIP6_pr/' + model_names[model] + MASK[model])\n",
    "    data2 = xr.open_dataset('/scratch/cimes/hh9736/CMIP6_pr/' + model_names[model] + MASK[model])\n",
    "    lats = data2['lat'].values\n",
    "    lats = lats.astype(int)\n",
    "    expolar = np.logical_or(lats > latuse, lats < -latuse)\n",
    "    \n",
    "    kn=round(lats.shape[0]/18)\n",
    "    \n",
    "    latmax=np.size(mat1['pr'][1,:,1])+1\n",
    "    lonmax=np.size(mat1['pr'][1,1,:])+1\n",
    "    lenlat=latmax-1\n",
    "    lenlon=lonmax-1\n",
    "    lenyear=2100-1850+1\n",
    "    miss_val = -9.99e08\n",
    "\n",
    "    lats = data2['lat'].values\n",
    "    lons = data2['lon'].values\n",
    "    weights = np.cos(np.deg2rad(lats))\n",
    "    weights /= np.sum(weights)\n",
    "    weights_reshaped = np.reshape(weights, (1, len(lats), 1))\n",
    "    weights_reshaped = np.repeat(weights_reshaped, len(lons), axis=2)\n",
    "\n",
    "\n",
    "# Specify the path to your NetCDF files\n",
    "    file_pattern = \"/scratch/cimes/hh9736/CMIP6_pr/\"+ model_names[model] +\"/pr*_r1i1p1f1_*.nc\"\n",
    "\n",
    "    # Get a list of file paths matching the pattern\n",
    "    file_paths = glob.glob(file_pattern)\n",
    "\n",
    "    # Open and concatenate the files one by one\n",
    "    datasets = []\n",
    "    for file_path in file_paths:\n",
    "        dataset = xr.open_dataset(file_path)\n",
    "        datasets.append(dataset)\n",
    "\n",
    "    # Concatenate the datasets along the time dimension\n",
    "    data = xr.concat(datasets, dim=\"time\")\n",
    "    data = data.sortby('time')\n",
    "    annual_mean = data.groupby('time.year').mean(dim='time')\n",
    "\n",
    "    # Select the first 100 years of data\n",
    "    start_year = annual_mean.year[0]\n",
    "    end_year = start_year + 99\n",
    "    YEARP = annual_mean.sel(year=slice(start_year, end_year))\n",
    "    mask=np.array(data1['mrsos'])\n",
    "    \n",
    "    mask=mask[0,:,:]\n",
    "    mask[mask>10000]=1.e+20\n",
    "    mask[mask<0.000000000000000000000001]=1.e+20\n",
    "   \n",
    "    weights_reshaped=weights_reshaped[0,:,:]\n",
    "    \n",
    "    mat1=YEARP['pr']\n",
    "    for yy in range(0,100):\n",
    "        pts=np.array(mat1[yy,:,:])\n",
    "  \n",
    "        la=spa_lag(pts, kn)\n",
    "        pts[expolar, :] = np.nan\n",
    "        \n",
    "        pts_land=copy(pts)\n",
    "        pts_land[mask>10000]=np.nan\n",
    "        GINIland[0,yy]=sgini(pts_land[~np.isnan(pts_land)], weights_reshaped[~np.isnan(pts_land)])\n",
    "        GINIsland[0,yy]=sgini_la(pts_land[~np.isnan(pts_land)],la[~np.isnan(pts_land)], weights_reshaped[~np.isnan(pts_land)])\n",
    "        \n",
    "        pts_ocean=copy(pts)\n",
    "        pts_ocean[mask<10000]=np.nan\n",
    "        GINIocean[0,yy]=sgini(pts_ocean[~np.isnan(pts_ocean)], weights_reshaped[~np.isnan(pts_ocean)])\n",
    "        GINIsocean[0,yy]=sgini_la(pts_ocean[~np.isnan(pts_ocean)],la[~np.isnan(pts_ocean)], weights_reshaped[~np.isnan(pts_ocean)])\n",
    "        \n",
    "        pts_glob=copy(pts)\n",
    "        GINIglob[0,yy]=sgini(pts_glob[~np.isnan(pts_glob)], weights_reshaped[~np.isnan(pts_glob)])\n",
    "        GINIsglob[0,yy]=sgini_la(pts_glob[~np.isnan(pts_glob)],la[~np.isnan(pts_glob)], weights_reshaped[~np.isnan(pts_glob)])\n",
    "        \n",
    "        \n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINI_spa_'+ str(latuse) +'landP_' + model_names[model] + '.npy', np.array([GINIland]))\n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINIs_spa_'+ str(latuse) +'landP_' + model_names[model] + '.npy', np.array([GINIsland]))\n",
    "    \n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINI_spa_'+ str(latuse) +'oceanP_' + model_names[model] + '.npy', np.array([GINIocean]))\n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINIs_spa_'+ str(latuse) +'oceanP_' + model_names[model] + '.npy', np.array([GINIsocean]))\n",
    "    \n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINI_spa_'+ str(latuse) +'globP_' + model_names[model] + '.npy', np.array([GINIglob]))\n",
    "    np.save('/scratch/cimes/hh9736/CMIP6_pr/GINIs_spa_'+ str(latuse) +'globP_' + model_names[model] + '.npy', np.array([GINIsglob]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96085dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cli-ana [~/.conda/envs/cli-ana/]",
   "language": "python",
   "name": "conda_cli-ana"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
